\documentclass[12pt,fleqn]{article}\usepackage{../common}
\begin{document}

\begin{minted}[fontsize=\footnotesize]{python}
import pandas as pd
import numpy as np
df = pd.read_csv('data/celeb_astro_mbti.csv',sep=';')
df['label'] = (df['mbti1']=='Ti').astype(float)
print list(df.columns)
\end{minted}

\begin{verbatim}
['mbti', 'name', 'spiller', 'chinese', 'lewi0', 'lewi1', 'lewi2', 'lewi3', 'lewi4', 'lewi5', 'lewi6', 'lewi7', 'lewi8', 'lewi9', 'lewi10', 'lewi11', 'lewi12', 'lewi13', 'lewi14', 'lewi15', 'lewi16', 'lewi17', 'lewi18', 'lewi19', 'lewi20', 'lewi21', 'lewi22', 'lewi23', 'lewi24', 'lewi25', 'lewi26', 'lewi27', 'lewi28', 'lewi29', 'lewi30', 'lewi31', 'lewi32', 'lewi33', 'lewi34', 'lewi35', 'lewi36', 'lewi37', 'lewi38', 'lewi39', 'lewi40', 'lewi41', 'lewi42', 'lewi43', 'lewi44', 'lewi45', 'lewi46', 'lewi47', 'lewi48', 'lewi49', 'lewi50', 'lewi51', 'lewi52', 'lewi53', 'lewi54', 'lewi55', 'lewi56', 'lewi57', 'lewi58', 'lewi59', 'lewi60', 'lewi61', 'lewi62', 'lewi63', 'lewi64', 'lewi65', 'lewi66', 'lewi67', 'lewi68', 'lewi69', 'lewi70', 'lewi71', 'lewi72', 'lewi73', 'lewi74', 'lewi75', 'lewi76', 'lewi77', 'lewi78', 'lewi79', 'lewi80', 'lewi81', 'lewi82', 'lewi83', 'lewi84', 'lewi85', 'lewi86', 'lewi87', 'lewi88', 'lewi89', 'lewi90', 'lewi91', 'lewi92', 'lewi93', 'lewi94', 'lewi95', 'lewi96', 'lewi97', 'lewi98', 'lewi99', 'lewi100', 'lewi101', 'lewi102', 'lewi103', 'lewi104', 'lewi105', 'lewi106', 'lewi107', 'lewi108', 'lewi109', 'lewi110', 'lewi111', 'lewi112', 'lewi113', 'lewi114', 'lewi115', 'lewi116', 'lewi117', 'lewi118', 'lewi119', 'lewi120', 'lewi121', 'lewi122', 'lewi123', 'lewi124', 'lewi125', 'lewi126', 'lewi127', 'lewi128', 'lewi129', 'lewi130', 'lewi131', 'lewi132', 'lewi133', 'lewi134', 'lewi135', 'lewi136', 'lewi137', 'lewi138', 'lewi139', 'lewi140', 'lewi141', 'lewi142', 'lewi143', 'lewi144', 'lewi145', 'lewi146', 'lewi147', 'lewi148', 'lewi149', 'lewi150', 'lewi151', 'lewi152', 'lewi153', 'lewi154', 'lewi155', 'lewi156', 'lewi157', 'lewi158', 'lewi159', 'lewi160', 'lewi161', 'lewi162', 'lewi163', 'lewi164', 'lewi165', 'lewi166', 'lewi167', 'lewi168', 'lewi169', 'lewi170', 'lewi171', 'lewi172', 'lewi173', 'lewi174', 'lewi175', 'lewi176', 'lewi177', 'lewi178', 'lewi179', 'lewi180', 'lewi181', 'lewi182', 'lewi183', 'lewi184', 'lewi185', 'lewi186', 'lewi187', 'lewi188', 'lewi189', 'lewi190', 'lewi191', 'lewi192', 'lewi193', 'lewi194', 'lewi195', 'lewi196', 'lewi197', 'lewi198', 'lewi199', 'lewi200', 'lewi201', 'lewi202', 'lewi203', 'lewi204', 'lewi205', 'lewi206', 'lewi207', 'lewi208', 'lewi209', 'lewi210', 'lewi211', 'lewi212', 'lewi213', 'lewi214', 'lewi215', 'lewi216', 'lewi217', 'lewi218', 'lewi219', 'lewi220', 'lewi221', 'lewi222', 'lewi223', 'lewi224', 'lewi225', 'lewi226', 'lewi227', 'lewi228', 'lewi229', 'lewi230', 'lewi231', 'lewi232', 'lewi233', 'lewi234', 'lewi235', 'lewi236', 'lewi237', 'lewi238', 'lewi239', 'lewi240', 'lewi241', 'lewi242', 'lewi243', 'lewi244', 'lewi245', 'lewi246', 'lewi247', 'lewi248', 'lewi249', 'lewi250', 'lewi251', 'lewi252', 'lewi253', 'lewi254', 'lewi255', 'lewi256', 'lewi257', 'lewi258', 'lewi259', 'lewi260', 'lewi261', 'lewi262', 'lewi263', 'lewi264', 'lewi265', 'lewi266', 'lewi267', 'lewi268', 'lewi269', 'lewi270', 'lewi271', 'lewi272', 'lewi273', 'lewi274', 'lewi275', 'lewi276', 'lewi277', 'M1', 'M2', 'mbti1', 'mbti2', 'label']
\end{verbatim}

\begin{minted}[fontsize=\footnotesize]{python}
logit = smf.logit("label ~ C(M1) + chinese + spiller", df).fit_regularized()
print logit.summary()
\end{minted}

\begin{verbatim}
Optimization terminated successfully.    (Exit mode 0)
            Current function value: 0.507213139772
            Iterations: 264
            Function evaluations: 264
            Gradient evaluations: 264
                           Logit Regression Results                           
==============================================================================
Dep. Variable:                  label   No. Observations:                  429
Model:                          Logit   Df Residuals:                      380
Method:                           MLE   Df Model:                           48
Date:                 So, 21 Feb 2016   Pseudo R-squ.:                  0.1239
Time:                        22:08:33   Log-Likelihood:                -217.59
converged:                       True   LL-Null:                       -248.38
                                        LLR p-value:                   0.09030
==========================================================================================
                             coef    std err          z      P>|z|      [95.0% Conf. Int.]
------------------------------------------------------------------------------------------
Intercept                 -0.5124      1.763     -0.291      0.771        -3.969     2.944
C(M1)[T.18]              -19.5328   8267.785     -0.002      0.998     -1.62e+04  1.62e+04
C(M1)[T.19]               -1.3267      1.791     -0.741      0.459        -4.838     2.184
C(M1)[T.20]               -2.5379      1.844     -1.377      0.169        -6.152     1.076
C(M1)[T.21]               -1.8754      2.004     -0.936      0.349        -5.804     2.053
C(M1)[T.22]               -0.3464      1.695     -0.204      0.838        -3.669     2.976
C(M1)[T.23]               -1.8072      1.770     -1.021      0.307        -5.276     1.661
C(M1)[T.24]               -2.6985      1.802     -1.497      0.134        -6.231     0.834
C(M1)[T.25]               -2.6236      1.714     -1.530      0.126        -5.984     0.736
C(M1)[T.26]               -2.1616      1.672     -1.293      0.196        -5.439     1.116
C(M1)[T.27]               -1.3293      1.681     -0.791      0.429        -4.624     1.965
C(M1)[T.28]               -1.7563      1.632     -1.076      0.282        -4.956     1.443
C(M1)[T.29]               -1.6668      1.659     -1.004      0.315        -4.919     1.586
C(M1)[T.30]               -2.3427      1.681     -1.393      0.164        -5.638     0.953
C(M1)[T.31]               -1.5243      1.638     -0.931      0.352        -4.734     1.685
C(M1)[T.32]               -1.7872      1.652     -1.082      0.279        -5.025     1.451
C(M1)[T.33]               -1.3199      1.661     -0.794      0.427        -4.576     1.936
C(M1)[T.34]               -1.4447      1.660     -0.870      0.384        -4.698     1.808
C(M1)[T.35]               -1.8061      1.714     -1.054      0.292        -5.166     1.554
C(M1)[T.36]                0.2779      1.695      0.164      0.870        -3.044     3.600
C(M1)[T.37]              -26.5435   1.28e+05     -0.000      1.000      -2.5e+05   2.5e+05
C(M1)[T.38]               -0.7902      1.734     -0.456      0.649        -4.189     2.609
C(M1)[T.39]               -2.3197      2.026     -1.145      0.252        -6.291     1.651
C(M1)[T.40]              -16.2303   2205.605     -0.007      0.994     -4339.137  4306.676
C(M1)[T.41]               -0.4412      2.233     -0.198      0.843        -4.817     3.935
C(M1)[T.42]              -18.2677   4765.643     -0.004      0.997     -9358.756  9322.220
C(M1)[T.43]               -1.2837      2.014     -0.637      0.524        -5.231     2.663
chinese[T.Dragon]          1.5924      0.653      2.437      0.015         0.312     2.873
chinese[T.Horse]           1.2757      0.706      1.807      0.071        -0.108     2.659
chinese[T.Monkey]          1.4885      0.710      2.096      0.036         0.097     2.880
chinese[T.Ox]              1.5747      0.670      2.351      0.019         0.262     2.888
chinese[T.Pig]             1.9106      0.655      2.915      0.004         0.626     3.195
chinese[T.Rabbit]          0.7050      0.707      0.997      0.319        -0.681     2.091
chinese[T.Rat]             1.1495      0.827      1.390      0.165        -0.471     2.770
chinese[T.Rooster]         0.6323      0.775      0.816      0.414        -0.886     2.150
chinese[T.Sheep]           0.5149      0.754      0.683      0.495        -0.964     1.994
chinese[T.Snake]           0.6045      0.674      0.897      0.370        -0.717     1.926
chinese[T.Tiger]           1.8315      0.664      2.758      0.006         0.530     3.133
spiller[T.Aries]          -0.1433      0.703     -0.204      0.838        -1.521     1.235
spiller[T.Cancer]         -0.0010      0.653     -0.002      0.999        -1.281     1.279
spiller[T.Capricorn]       0.2717      0.651      0.418      0.676        -1.003     1.547
spiller[T.Gemini]          0.0313      0.668      0.047      0.963        -1.279     1.342
spiller[T.Leo]            -0.2645      0.685     -0.386      0.699        -1.608     1.079
spiller[T.Libra]           1.0874      0.616      1.764      0.078        -0.121     2.296
spiller[T.Pisces]         -0.1190      0.674     -0.176      0.860        -1.441     1.203
spiller[T.Sagittarius]    -0.4001      0.661     -0.605      0.545        -1.697     0.896
spiller[T.Scorpio]        -0.0023      0.730     -0.003      0.997        -1.434     1.429
spiller[T.Taurus]         -0.3740      0.654     -0.571      0.568        -1.657     0.909
spiller[T.Virgo]          -0.2589      0.722     -0.359      0.720        -1.674     1.156
==========================================================================================
\end{verbatim}


######################################################################3
######################################################################3
######################################################################3
######################################################################3
######################################################################3


\begin{minted}[fontsize=\footnotesize]{python}
from sklearn.feature_extraction import DictVectorizer
import random

def one_hot_dataframe(data, cols):
    vec = DictVectorizer()
    mkdict = lambda row: dict((col, row[col]) for col in cols)
    tmp = vec.fit_transform(data[cols].to_dict(outtype='records')).toarray()
    vecData = pd.DataFrame(tmp)
    vecData.columns = vec.get_feature_names()
    vecData.index = data.index
    data = data.drop(cols, axis=1)
    data = data.join(vecData)
    return data

df2 = df.drop(['label','name','mbti'],axis=1)
df2['M1'] = df2.M1.astype(str)
df2['M2'] = df2.M2.astype(str)
X = one_hot_dataframe(df2,['M1','M2','spiller','chinese'])
y = X[['mbti1','mbti2']]
X = X.drop(['mbti1','mbti2'],axis=1)

X2 = np.array(X.fillna(0))
y2 = np.array(y.mbti1=='Ni')
print list(y.columns)
print list(X.columns)
print y2.shape, X2.shape
\end{minted}

\begin{verbatim}
['mbti1', 'mbti2']
['lewi0', 'lewi1', 'lewi2', 'lewi3', 'lewi4', 'lewi5', 'lewi6', 'lewi7', 'lewi8', 'lewi9', 'lewi10', 'lewi11', 'lewi12', 'lewi13', 'lewi14', 'lewi15', 'lewi16', 'lewi17', 'lewi18', 'lewi19', 'lewi20', 'lewi21', 'lewi22', 'lewi23', 'lewi24', 'lewi25', 'lewi26', 'lewi27', 'lewi28', 'lewi29', 'lewi30', 'lewi31', 'lewi32', 'lewi33', 'lewi34', 'lewi35', 'lewi36', 'lewi37', 'lewi38', 'lewi39', 'lewi40', 'lewi41', 'lewi42', 'lewi43', 'lewi44', 'lewi45', 'lewi46', 'lewi47', 'lewi48', 'lewi49', 'lewi50', 'lewi51', 'lewi52', 'lewi53', 'lewi54', 'lewi55', 'lewi56', 'lewi57', 'lewi58', 'lewi59', 'lewi60', 'lewi61', 'lewi62', 'lewi63', 'lewi64', 'lewi65', 'lewi66', 'lewi67', 'lewi68', 'lewi69', 'lewi70', 'lewi71', 'lewi72', 'lewi73', 'lewi74', 'lewi75', 'lewi76', 'lewi77', 'lewi78', 'lewi79', 'lewi80', 'lewi81', 'lewi82', 'lewi83', 'lewi84', 'lewi85', 'lewi86', 'lewi87', 'lewi88', 'lewi89', 'lewi90', 'lewi91', 'lewi92', 'lewi93', 'lewi94', 'lewi95', 'lewi96', 'lewi97', 'lewi98', 'lewi99', 'lewi100', 'lewi101', 'lewi102', 'lewi103', 'lewi104', 'lewi105', 'lewi106', 'lewi107', 'lewi108', 'lewi109', 'lewi110', 'lewi111', 'lewi112', 'lewi113', 'lewi114', 'lewi115', 'lewi116', 'lewi117', 'lewi118', 'lewi119', 'lewi120', 'lewi121', 'lewi122', 'lewi123', 'lewi124', 'lewi125', 'lewi126', 'lewi127', 'lewi128', 'lewi129', 'lewi130', 'lewi131', 'lewi132', 'lewi133', 'lewi134', 'lewi135', 'lewi136', 'lewi137', 'lewi138', 'lewi139', 'lewi140', 'lewi141', 'lewi142', 'lewi143', 'lewi144', 'lewi145', 'lewi146', 'lewi147', 'lewi148', 'lewi149', 'lewi150', 'lewi151', 'lewi152', 'lewi153', 'lewi154', 'lewi155', 'lewi156', 'lewi157', 'lewi158', 'lewi159', 'lewi160', 'lewi161', 'lewi162', 'lewi163', 'lewi164', 'lewi165', 'lewi166', 'lewi167', 'lewi168', 'lewi169', 'lewi170', 'lewi171', 'lewi172', 'lewi173', 'lewi174', 'lewi175', 'lewi176', 'lewi177', 'lewi178', 'lewi179', 'lewi180', 'lewi181', 'lewi182', 'lewi183', 'lewi184', 'lewi185', 'lewi186', 'lewi187', 'lewi188', 'lewi189', 'lewi190', 'lewi191', 'lewi192', 'lewi193', 'lewi194', 'lewi195', 'lewi196', 'lewi197', 'lewi198', 'lewi199', 'lewi200', 'lewi201', 'lewi202', 'lewi203', 'lewi204', 'lewi205', 'lewi206', 'lewi207', 'lewi208', 'lewi209', 'lewi210', 'lewi211', 'lewi212', 'lewi213', 'lewi214', 'lewi215', 'lewi216', 'lewi217', 'lewi218', 'lewi219', 'lewi220', 'lewi221', 'lewi222', 'lewi223', 'lewi224', 'lewi225', 'lewi226', 'lewi227', 'lewi228', 'lewi229', 'lewi230', 'lewi231', 'lewi232', 'lewi233', 'lewi234', 'lewi235', 'lewi236', 'lewi237', 'lewi238', 'lewi239', 'lewi240', 'lewi241', 'lewi242', 'lewi243', 'lewi244', 'lewi245', 'lewi246', 'lewi247', 'lewi248', 'lewi249', 'lewi250', 'lewi251', 'lewi252', 'lewi253', 'lewi254', 'lewi255', 'lewi256', 'lewi257', 'lewi258', 'lewi259', 'lewi260', 'lewi261', 'lewi262', 'lewi263', 'lewi264', 'lewi265', 'lewi266', 'lewi267', 'lewi268', 'lewi269', 'lewi270', 'lewi271', 'lewi272', 'lewi273', 'lewi274', 'lewi275', 'lewi276', 'lewi277', 'M1=15', 'M1=18', 'M1=19', 'M1=20', 'M1=21', 'M1=22', 'M1=23', 'M1=24', 'M1=25', 'M1=26', 'M1=27', 'M1=28', 'M1=29', 'M1=30', 'M1=31', 'M1=32', 'M1=33', 'M1=34', 'M1=35', 'M1=36', 'M1=37', 'M1=38', 'M1=39', 'M1=40', 'M1=41', 'M1=42', 'M1=43', 'M2=10', 'M2=11', 'M2=12', 'M2=2', 'M2=3', 'M2=4', 'M2=5', 'M2=6', 'M2=7', 'M2=8', 'M2=9', 'chinese', 'chinese=Dog', 'chinese=Dragon', 'chinese=Horse', 'chinese=Monkey', 'chinese=Ox', 'chinese=Pig', 'chinese=Rabbit', 'chinese=Rat', 'chinese=Rooster', 'chinese=Sheep', 'chinese=Snake', 'chinese=Tiger', 'spiller=Aquarius', 'spiller=Aries', 'spiller=Cancer', 'spiller=Capricorn', 'spiller=Gemini', 'spiller=Leo', 'spiller=Libra', 'spiller=Pisces', 'spiller=Sagittarius', 'spiller=Scorpio', 'spiller=Taurus', 'spiller=Virgo']
(431,) (431, 341)
\end{verbatim}

\begin{minted}[fontsize=\footnotesize]{python}
\end{minted}

\begin{verbatim}
(129, 341)
\end{verbatim}

\begin{minted}[fontsize=\footnotesize]{python}
import statsmodels.api as sm
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import roc_curve, auc
from sklearn.metrics import roc_auc_score
from sklearn.cross_validation import train_test_split

X_train,X_test,y_train,y_test = train_test_split(X2,y2,test_size=0.6,random_state=0)

print X_train.shape
acc=0.00000001
L1_ALPHA = 16.0
alpha=L1_ALPHA
clf = sm.Logit(y_train, X_train).fit_regularized(maxiter=1024, alpha=alpha, acc=acc, disp=False)
pred = clf.predict(X_test)
\end{minted}

\begin{verbatim}
(172, 341)
\end{verbatim}

\begin{minted}[fontsize=\footnotesize]{python}
fpr, tpr, thresholds = roc_curve(y_test, pred)
roc_auc = auc(fpr, tpr)
print 'AUC', roc_auc
\end{minted}

\begin{verbatim}
AUC 0.56445035461
\end{verbatim}

\begin{minted}[fontsize=\footnotesize]{python}
print pred
\end{minted}

\begin{verbatim}
[ 0.26532895  0.26532895  0.5         0.5         0.5         0.26532895
  0.5         0.5         0.5         0.5         0.5         0.26532895
  0.26532895  0.26532895  0.5         0.26532895  0.26532895  0.26532895
  0.5         0.26532895  0.5         0.5         0.5         0.26532895
  0.5         0.5         0.5         0.26532895  0.26532895  0.26532895
  0.26532895  0.26532895  0.26532895  0.26532895  0.26532895  0.5         0.5
  0.26532895  0.26532895  0.5         0.26532895  0.26532895  0.26532895
  0.26532895  0.26532895  0.5         0.26532895  0.26532895  0.5
  0.26532895  0.26532895  0.26532895  0.5         0.26532895  0.5
  0.26532895  0.5         0.26532895  0.26532895  0.5         0.5         0.5
  0.26532895  0.5         0.5         0.26532895  0.5         0.26532895
  0.26532895  0.5         0.5         0.5         0.5         0.26532895
  0.26532895  0.5         0.26532895  0.26532895  0.26532895  0.5         0.5
  0.26532895  0.26532895  0.5         0.5         0.5         0.26532895
  0.26532895  0.5         0.5         0.5         0.26532895  0.26532895
  0.26532895  0.5         0.26532895  0.26532895  0.5         0.26532895
  0.26532895  0.5         0.26532895  0.26532895  0.5         0.5
  0.26532895  0.26532895  0.26532895  0.5         0.26532895  0.26532895
  0.5         0.5         0.5         0.5         0.26532895  0.26532895
  0.26532895  0.26532895  0.26532895  0.5         0.5         0.26532895
  0.5         0.26532895  0.5         0.26532895  0.5         0.26532895
  0.26532895  0.26532895  0.26532895  0.26532895  0.5         0.26532895
  0.26532895  0.5         0.26532895  0.5         0.5         0.5
  0.26532895  0.26532895  0.5         0.26532895  0.26532895  0.5
  0.26532895  0.26532895  0.5         0.26532895  0.26532895  0.5
  0.26532895  0.26532895  0.26532895  0.5         0.26532895  0.26532895
  0.5         0.5         0.5         0.5         0.5         0.5
  0.26532895  0.5         0.5         0.5         0.26532895  0.26532895
  0.5         0.26532895  0.26532895  0.5         0.5         0.5
  0.26532895  0.26532895  0.5         0.5         0.5         0.26532895
  0.5         0.26532895  0.5         0.26532895  0.5         0.26532895
  0.26532895  0.26532895  0.26532895  0.26532895  0.5         0.5
  0.26532895  0.5         0.26532895  0.26532895  0.5         0.5         0.5
  0.26532895  0.26532895  0.5         0.5         0.26532895  0.26532895
  0.5         0.26532895  0.5         0.26532895  0.26532895  0.26532895
  0.26532895  0.26532895  0.5         0.26532895  0.5         0.26532895
  0.26532895  0.26532895  0.26532895  0.26532895  0.26532895  0.26532895
  0.26532895  0.26532895  0.26532895  0.26532895  0.26532895  0.5         0.5
  0.5         0.26532895  0.26532895  0.5         0.26532895  0.5
  0.26532895  0.5         0.26532895  0.5         0.26532895  0.5
  0.26532895  0.26532895  0.26532895  0.5         0.5         0.26532895
  0.5         0.26532895  0.26532895  0.26532895  0.26532895  0.26532895
  0.26532895  0.26532895]
\end{verbatim}



\end{document}
